{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d23103d",
   "metadata": {},
   "source": [
    "# Topic Modeling on News Snippets Using LDA\n",
    "**Author:** Virginia Herrero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d8b69",
   "metadata": {},
   "source": [
    "## Import Libraries and Download Resources\n",
    "\n",
    "Import essential libraries for text preprocessing, topic modeling, and download required NLTK resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b893e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Topic Modeling\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaMulticore, CoherenceModel\n",
    "\n",
    "# Utilities\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31ce73",
   "metadata": {},
   "source": [
    "## Define the Corpus\n",
    "\n",
    "In natural language processing, a corpus is a collection of written or spoken texts that serves as the dataset for language-related tasks. The corpus is analyzed to identify language patterns and typically requires preprocessing and transformation into a format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62967e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The stock market closed higher today as tech shares rallied amid strong earnings reports.\",\n",
    "    \"A major earthquake struck the coastal city early this morning, causing widespread damage.\",\n",
    "    \"The government announced new policies aimed at reducing carbon emissions by 2030.\",\n",
    "    \"Scientists discovered a new species of dinosaur in the remote mountains of Argentina.\",\n",
    "    \"The local football team won the championship after a thrilling final match.\",\n",
    "    \"Health officials urge citizens to get vaccinated as flu season approaches.\",\n",
    "    \"A breakthrough in renewable energy technology promises cheaper solar panels.\",\n",
    "    \"International leaders met to discuss trade agreements and economic cooperation.\",\n",
    "    \"A popular film festival opened this weekend, showcasing independent movies from around the world.\",\n",
    "    \"The city council approved plans for a new public park to promote green spaces.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e0d46",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "\n",
    "After defining the corpus, the next step is text preprocessing. This step involves cleaning and preparing the raw text data to make it suitable for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94420aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stopwords\n",
    "stop_w = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "def doc_to_tokens(texts):\n",
    "    \"\"\"\n",
    "    Tokenize a list of documents into clean lowercase words.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    texts (list of str): List of raw text documents.\n",
    "\n",
    "    Yields:\n",
    "    ----------\n",
    "    list of str: Tokenized and lowercased words from each document,\n",
    "                 with punctuation removed.\n",
    "    \"\"\"\n",
    "    for doc in texts:\n",
    "        yield simple_preprocess(doc, deacc=True)\n",
    "\n",
    "tokens = list(doc_to_tokens(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9faa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "def rm_stopwords(docs):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from tokenized documents.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    docs (list of list of str): Tokenized documents (list of words).\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    list of list of str: Tokenized documents with stopwords removed.\n",
    "    \"\"\"\n",
    "    return [[word for word in doc if word not in stop_w] for doc in docs]\n",
    "\n",
    "tokens = rm_stopwords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df07da7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed documents:\n",
      "[['stock', 'market', 'close', 'higher', 'today', 'tech', 'share', 'ralli', 'amid', 'strong', 'earn', 'report'], ['major', 'earthquak', 'struck', 'coastal', 'citi', 'earli', 'morn', 'caus', 'widespread', 'damag']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Lemmatize and stem tokens\n",
    "docs = []\n",
    "for doc in tokens:\n",
    "    word_list = []\n",
    "    for token in doc:\n",
    "        lemm = lemmatizer.lemmatize(token)\n",
    "        stem = stemmer.stem(lemm)\n",
    "        word_list.append(stem)\n",
    "    docs.append(word_list)\n",
    "\n",
    "print(\"Sample processed documents:\")\n",
    "print(docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913bfee",
   "metadata": {},
   "source": [
    "## Create Dictionary\n",
    "\n",
    "A dictionary in natural language processing is a mapping between unique words (tokens) in the corpus and their integer IDs. It serves as a vocabulary reference that converts text data into numerical formats required by machine learning models. In topic modeling, the dictionary helps translate words into a consistent numeric representation used to build the corpus and train models like LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c323aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dictionary token-id pairs:\n",
      "[(0, 'amid'), (1, 'close'), (2, 'earn'), (3, 'higher'), (4, 'market'), (5, 'ralli'), (6, 'report'), (7, 'share'), (8, 'stock'), (9, 'strong')]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary representation of the documents\n",
    "word_dict = corpora.Dictionary(docs)\n",
    "\n",
    "# Print the first 10 token-id\n",
    "print(\"Sample dictionary token-id pairs:\")\n",
    "print(list(word_dict.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29984f8",
   "metadata": {},
   "source": [
    "## Create Bag-of-Words\n",
    "\n",
    "A bag of words (BoW) is a simple and commonly used method for representing text data in natural language processing. It treats a document as a \"bag\" of individual words, ignoring grammar and word order, but keeping track of how many times each word appears. Each document is converted into a vector of word counts based on a predefined vocabulary. In conclusion, a bag of words is a numerical representation of text that captures word frequency, used to feed text data into machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4aadfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample bag-of-words representation for first document:\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create the bag-of-words corpus\n",
    "bow_corpus = [word_dict.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# Print the bag-of-words for the first document\n",
    "print(\"Sample bag-of-words representation for first document:\")\n",
    "print(bow_corpus[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
