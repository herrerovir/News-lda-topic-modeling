{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d23103d",
   "metadata": {},
   "source": [
    "# Topic Modeling on News Snippets Using LDA\n",
    "**Author:** Virginia Herrero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d8b69",
   "metadata": {},
   "source": [
    "## Import Libraries and Download Resources\n",
    "\n",
    "Import essential libraries for text preprocessing, topic modeling, and download required NLTK resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b893e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Topic Modeling\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaMulticore, CoherenceModel\n",
    "\n",
    "# Utilities\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31ce73",
   "metadata": {},
   "source": [
    "## Define the Corpus\n",
    "\n",
    "In natural language processing, a corpus is a collection of written or spoken texts that serves as the dataset for language-related tasks. The corpus is analyzed to identify language patterns and typically requires preprocessing and transformation into a format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62967e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The stock market closed higher today as tech shares rallied amid strong earnings reports.\",\n",
    "    \"A major earthquake struck the coastal city early this morning, causing widespread damage.\",\n",
    "    \"The government announced new policies aimed at reducing carbon emissions by 2030.\",\n",
    "    \"Scientists discovered a new species of dinosaur in the remote mountains of Argentina.\",\n",
    "    \"The local football team won the championship after a thrilling final match.\",\n",
    "    \"Health officials urge citizens to get vaccinated as flu season approaches.\",\n",
    "    \"A breakthrough in renewable energy technology promises cheaper solar panels.\",\n",
    "    \"International leaders met to discuss trade agreements and economic cooperation.\",\n",
    "    \"A popular film festival opened this weekend, showcasing independent movies from around the world.\",\n",
    "    \"The city council approved plans for a new public park to promote green spaces.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e0d46",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "\n",
    "After defining the corpus, the next step is text preprocessing. This step involves cleaning and preparing the raw text data to make it suitable for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94420aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stopwords\n",
    "stop_w = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1842f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "def doc_to_tokens(texts):\n",
    "    \"\"\"\n",
    "    Tokenize a list of documents into clean lowercase words.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    texts (list of str): List of raw text documents.\n",
    "\n",
    "    Yields:\n",
    "    ----------\n",
    "    list of str: Tokenized and lowercased words from each document,\n",
    "                 with punctuation removed.\n",
    "    \"\"\"\n",
    "    for doc in texts:\n",
    "        yield simple_preprocess(doc, deacc=True)\n",
    "\n",
    "tokens = list(doc_to_tokens(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e9faa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "def rm_stopwords(docs):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from tokenized documents.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    docs (list of list of str): Tokenized documents (list of words).\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    list of list of str: Tokenized documents with stopwords removed.\n",
    "    \"\"\"\n",
    "    return [[word for word in doc if word not in stop_w] for doc in docs]\n",
    "\n",
    "tokens = rm_stopwords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df07da7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed documents:\n",
      "[['stock', 'market', 'close', 'higher', 'today', 'tech', 'share', 'ralli', 'amid', 'strong', 'earn', 'report'], ['major', 'earthquak', 'struck', 'coastal', 'citi', 'earli', 'morn', 'caus', 'widespread', 'damag']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Lemmatize and stem tokens\n",
    "docs = []\n",
    "for doc in tokens:\n",
    "    word_list = []\n",
    "    for token in doc:\n",
    "        lemm = lemmatizer.lemmatize(token)\n",
    "        stem = stemmer.stem(lemm)\n",
    "        word_list.append(stem)\n",
    "    docs.append(word_list)\n",
    "\n",
    "print(\"Sample processed documents:\")\n",
    "print(docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913bfee",
   "metadata": {},
   "source": [
    "## Create Dictionary\n",
    "\n",
    "A dictionary in natural language processing is a mapping between unique words (tokens) in the corpus and their integer IDs. It serves as a vocabulary reference that converts text data into numerical formats required by machine learning models. In topic modeling, the dictionary helps translate words into a consistent numeric representation used to build the corpus and train models like LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24c323aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dictionary token-id pairs:\n",
      "[(0, 'amid'), (1, 'close'), (2, 'earn'), (3, 'higher'), (4, 'market'), (5, 'ralli'), (6, 'report'), (7, 'share'), (8, 'stock'), (9, 'strong')]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary representation of the documents\n",
    "word_dict = corpora.Dictionary(docs)\n",
    "\n",
    "# Print the first 10 token-id\n",
    "print(\"Sample dictionary token-id pairs:\")\n",
    "print(list(word_dict.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29984f8",
   "metadata": {},
   "source": [
    "## Create Bag-of-Words\n",
    "\n",
    "A bag of words (BoW) is a simple and commonly used method for representing text data in natural language processing. It treats a document as a \"bag\" of individual words, ignoring grammar and word order, but keeping track of how many times each word appears. Each document is converted into a vector of word counts based on a predefined vocabulary. In conclusion, a bag of words is a numerical representation of text that captures word frequency, used to feed text data into machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f4aadfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample bag-of-words representation for first document:\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create the bag-of-words corpus\n",
    "bow_corpus = [word_dict.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# Print the bag-of-words for the first document\n",
    "print(\"Sample bag-of-words representation for first document:\")\n",
    "print(bow_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb756be0",
   "metadata": {},
   "source": [
    "## LDA Model\n",
    "\n",
    "Topic modeling is the process of uncovering hidden thematic structures in a collection of documents. LDA (Latent Dirichlet Allocation) is one of the most commonly used algorithms for this task. It identifies groups of words that frequently occur together and uses them to define topics, allowing each document to be represented as a mixture of these topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0cded99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.038*\"new\" + 0.022*\"argentina\" + 0.022*\"mountain\" + 0.022*\"scientist\" + '\n",
      "  '0.022*\"remot\" + 0.022*\"discov\" + 0.022*\"dinosaur\" + 0.022*\"speci\" + '\n",
      "  '0.021*\"reduc\" + 0.021*\"emiss\"'),\n",
      " (1,\n",
      "  '0.034*\"citi\" + 0.020*\"space\" + 0.020*\"public\" + 0.020*\"promot\" + '\n",
      "  '0.020*\"approv\" + 0.020*\"plan\" + 0.020*\"green\" + 0.020*\"council\" + '\n",
      "  '0.020*\"park\" + 0.020*\"strong\"'),\n",
      " (2,\n",
      "  '0.028*\"film\" + 0.028*\"weekend\" + 0.028*\"showcas\" + 0.028*\"open\" + '\n",
      "  '0.028*\"popular\" + 0.028*\"world\" + 0.028*\"independ\" + 0.028*\"around\" + '\n",
      "  '0.028*\"movi\" + 0.028*\"festiv\"')]\n"
     ]
    }
   ],
   "source": [
    "# Train the LDA model\n",
    "lda_model = LdaMulticore(\n",
    "    corpus = bow_corpus,        # The BoW representation of the documents\n",
    "    id2word = word_dict,        # The dictionary mapping of word IDs\n",
    "    num_topics = 3,             # Number of topics to extract\n",
    "    random_state = 42,          # For reproducibility\n",
    "    passes = 10,                # Number of passes through the corpus during training\n",
    ")\n",
    "\n",
    "# Display the discovered topics\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics(num_words = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2f1f0",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Model evaluation measures how well a topic model produces coherent and meaningful topics. This process ensures the model’s results are reliable and interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "090b13cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.2541\n"
     ]
    }
   ],
   "source": [
    "# Build coherence model\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model = lda_model, \n",
    "    texts = docs,          \n",
    "    dictionary = word_dict,\n",
    "    coherence = \"c_v\"      \n",
    ")\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "\n",
    "print(f\"Coherence Score: {coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8e4d0",
   "metadata": {},
   "source": [
    "The model’s coherence score is quite low, so the next step is to train multiple LDA models using different numbers of topics and calculate their coherence scores. By comparing these scores, it finds the optimal number of topics that improves the model’s coherence and overall quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "224877e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple LDA models to find the best\n",
    "def compute_coherence_values(dictionary, corpus, texts, start = 2, limit = 10, step = 1):\n",
    "    \"\"\"\n",
    "    Train LDA models with varying number of topics and compute their coherence scores.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    - dictionary (gensim.corpora.Dictionary): Mapping of word IDs to words.\n",
    "    - corpus (list of list of (int, int)): Bag-of-words representation of documents.\n",
    "    - texts (list of list of str): Preprocessed tokenized documents.\n",
    "    - start (int): Minimum number of topics to try (inclusive).\n",
    "    - limit (int): Maximum number of topics to try (inclusive).\n",
    "    - step (int): Step size between topic numbers.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    - model_list (list): List of trained LDA models.\n",
    "    - coherence_values (list): List of coherence scores corresponding to each model.\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, limit + 1, step):\n",
    "        print(f\"Training LDA with {num_topics} topics...\")\n",
    "        model = LdaMulticore(\n",
    "            corpus = corpus,\n",
    "            id2word = dictionary,\n",
    "            num_topics = num_topics,\n",
    "            random_state = 42,\n",
    "            passes = 10,\n",
    "        )\n",
    "        model_list.append(model)\n",
    "        \n",
    "        coherence_model = CoherenceModel(\n",
    "            model = model,\n",
    "            texts = texts,\n",
    "            dictionary = dictionary,\n",
    "            coherence = \"c_v\"\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        coherence_values.append(coherence_score)\n",
    "        \n",
    "        print(f\"Coherence Score for {num_topics} topics: {coherence_score:.4f}\\n\")\n",
    "    \n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54b9db36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA with 2 topics...\n",
      "Coherence Score for 2 topics: 0.2105\n",
      "\n",
      "Training LDA with 3 topics...\n",
      "Coherence Score for 3 topics: 0.2541\n",
      "\n",
      "Training LDA with 4 topics...\n",
      "Coherence Score for 4 topics: 0.2814\n",
      "\n",
      "Training LDA with 5 topics...\n",
      "Coherence Score for 5 topics: 0.2298\n",
      "\n",
      "Training LDA with 6 topics...\n",
      "Coherence Score for 6 topics: 0.3579\n",
      "\n",
      "Training LDA with 7 topics...\n",
      "Coherence Score for 7 topics: 0.3620\n",
      "\n",
      "Training LDA with 8 topics...\n",
      "Coherence Score for 8 topics: 0.4415\n",
      "\n",
      "Training LDA with 9 topics...\n",
      "Coherence Score for 9 topics: 0.4346\n",
      "\n",
      "Training LDA with 10 topics...\n",
      "Coherence Score for 10 topics: 0.4009\n",
      "\n",
      "Num Topics = 2 => Coherence Score = 0.2105\n",
      "Num Topics = 3 => Coherence Score = 0.2541\n",
      "Num Topics = 4 => Coherence Score = 0.2814\n",
      "Num Topics = 5 => Coherence Score = 0.2298\n",
      "Num Topics = 6 => Coherence Score = 0.3579\n",
      "Num Topics = 7 => Coherence Score = 0.3620\n",
      "Num Topics = 8 => Coherence Score = 0.4415\n",
      "Num Topics = 9 => Coherence Score = 0.4346\n",
      "Num Topics = 10 => Coherence Score = 0.4009\n"
     ]
    }
   ],
   "source": [
    "# Run coherence evaluation for topics 2 to 10\n",
    "model_list, coherence_values = compute_coherence_values(word_dict, bow_corpus, docs, start = 2, limit = 10, step = 1)\n",
    "\n",
    "# Print summary\n",
    "for num, score in zip(range(2, 11), coherence_values):\n",
    "    print(f\"Num Topics = {num} => Coherence Score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75530b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LDA Model has 8 topics with coherence score 0.4415\n",
      "\n",
      "[(0,\n",
      "  '0.011*\"new\" + 0.011*\"local\" + 0.011*\"remot\" + 0.011*\"renew\" + 0.011*\"match\" '\n",
      "  '+ 0.011*\"scientist\" + 0.011*\"citi\" + 0.011*\"team\" + 0.011*\"govern\" + '\n",
      "  '0.011*\"breakthrough\" + 0.011*\"polici\" + 0.011*\"discu\" + 0.011*\"citizen\" + '\n",
      "  '0.011*\"approach\" + 0.011*\"reduc\" + 0.011*\"emiss\" + 0.011*\"final\" + '\n",
      "  '0.011*\"carbon\" + 0.011*\"argentina\" + 0.011*\"promis\"'),\n",
      " (1,\n",
      "  '0.011*\"new\" + 0.011*\"team\" + 0.011*\"citi\" + 0.011*\"final\" + 0.011*\"local\" + '\n",
      "  '0.011*\"championship\" + 0.011*\"thrill\" + 0.011*\"panel\" + 0.011*\"renew\" + '\n",
      "  '0.011*\"season\" + 0.011*\"scientist\" + 0.011*\"match\" + 0.011*\"govern\" + '\n",
      "  '0.011*\"remot\" + 0.011*\"approach\" + 0.011*\"footbal\" + 0.011*\"emiss\" + '\n",
      "  '0.011*\"promis\" + 0.011*\"polici\" + 0.011*\"breakthrough\"'),\n",
      " (2,\n",
      "  '0.054*\"weekend\" + 0.054*\"open\" + 0.054*\"independ\" + 0.054*\"film\" + '\n",
      "  '0.054*\"around\" + 0.054*\"showcas\" + 0.054*\"popular\" + 0.054*\"movi\" + '\n",
      "  '0.054*\"world\" + 0.054*\"festiv\" + 0.006*\"new\" + 0.006*\"renew\" + '\n",
      "  '0.006*\"breakthrough\" + 0.006*\"remot\" + 0.006*\"promis\" + 0.006*\"citi\" + '\n",
      "  '0.006*\"discu\" + 0.006*\"local\" + 0.006*\"scientist\" + 0.006*\"health\"'),\n",
      " (3,\n",
      "  '0.011*\"new\" + 0.011*\"remot\" + 0.011*\"renew\" + 0.011*\"citi\" + '\n",
      "  '0.011*\"approach\" + 0.011*\"promis\" + 0.011*\"local\" + 0.011*\"polici\" + '\n",
      "  '0.011*\"discov\" + 0.011*\"mountain\" + 0.011*\"match\" + 0.011*\"championship\" + '\n",
      "  '0.011*\"season\" + 0.011*\"final\" + 0.011*\"urg\" + 0.011*\"cheaper\" + '\n",
      "  '0.011*\"panel\" + 0.011*\"breakthrough\" + 0.011*\"argentina\" + 0.011*\"reduc\"'),\n",
      " (4,\n",
      "  '0.011*\"new\" + 0.011*\"remot\" + 0.011*\"team\" + 0.011*\"match\" + '\n",
      "  '0.011*\"argentina\" + 0.011*\"citi\" + 0.011*\"renew\" + 0.011*\"panel\" + '\n",
      "  '0.011*\"local\" + 0.011*\"final\" + 0.011*\"scientist\" + 0.011*\"energi\" + '\n",
      "  '0.011*\"cheaper\" + 0.011*\"championship\" + 0.011*\"discov\" + 0.011*\"coastal\" + '\n",
      "  '0.011*\"govern\" + 0.011*\"intern\" + 0.011*\"discu\" + 0.011*\"breakthrough\"'),\n",
      " (5,\n",
      "  '0.069*\"citi\" + 0.036*\"new\" + 0.036*\"space\" + 0.036*\"earli\" + 0.036*\"approv\" '\n",
      "  '+ 0.036*\"council\" + 0.036*\"morn\" + 0.036*\"park\" + 0.036*\"plan\" + '\n",
      "  '0.036*\"promot\" + 0.036*\"major\" + 0.036*\"widespread\" + 0.036*\"public\" + '\n",
      "  '0.036*\"damag\" + 0.036*\"struck\" + 0.036*\"earthquak\" + 0.036*\"green\" + '\n",
      "  '0.036*\"caus\" + 0.036*\"coastal\" + 0.004*\"renew\"'),\n",
      " (6,\n",
      "  '0.036*\"stock\" + 0.036*\"ralli\" + 0.036*\"amid\" + 0.036*\"close\" + '\n",
      "  '0.036*\"today\" + 0.036*\"share\" + 0.036*\"strong\" + 0.036*\"market\" + '\n",
      "  '0.036*\"higher\" + 0.036*\"earn\" + 0.036*\"tech\" + 0.036*\"report\" + '\n",
      "  '0.036*\"leader\" + 0.036*\"trade\" + 0.036*\"econom\" + 0.036*\"cooper\" + '\n",
      "  '0.036*\"met\" + 0.036*\"agreement\" + 0.036*\"intern\" + 0.036*\"discu\"'),\n",
      " (7,\n",
      "  '0.042*\"new\" + 0.022*\"get\" + 0.022*\"offici\" + 0.022*\"vaccin\" + '\n",
      "  '0.022*\"health\" + 0.022*\"citizen\" + 0.022*\"energi\" + 0.022*\"flu\" + '\n",
      "  '0.022*\"urg\" + 0.022*\"dinosaur\" + 0.022*\"speci\" + 0.022*\"aim\" + '\n",
      "  '0.022*\"approach\" + 0.022*\"announc\" + 0.022*\"season\" + 0.022*\"polici\" + '\n",
      "  '0.022*\"cheaper\" + 0.022*\"emiss\" + 0.022*\"reduc\" + 0.022*\"discov\"')]\n"
     ]
    }
   ],
   "source": [
    "# The best model\n",
    "# Find the index of the best coherence score\n",
    "best_model_index = coherence_values.index(max(coherence_values))\n",
    "\n",
    "# Select the best model\n",
    "best_model = model_list[best_model_index]\n",
    "\n",
    "# Print summary of the best model\n",
    "print(f\"\\nBest LDA Model has {best_model.num_topics} topics with coherence score {coherence_values[best_model_index]:.4f}\\n\")\n",
    "\n",
    "# Pretty-print the topics of the best model\n",
    "pprint(best_model.print_topics(num_words = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196af62e",
   "metadata": {},
   "source": [
    "The model was tested with different numbers of topics, and the quality of the topics improved as the number increased, peaking at 8 topics with the best coherence score of 0.4415. This means the model found the most meaningful and distinct themes when using 8 topics. The topics include groups of related words representing different themes like environment, finance, and health. While the score shows the model captures some clear patterns, there is still room to improve the results with further tuning or preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1ef7d",
   "metadata": {},
   "source": [
    "## Extract Document Topic Distributions\n",
    "\n",
    "The topic distribution shows how much each topic contributes to a given document. After training the LDA model, each document is represented as a mixture of topics with associated probabilities. This helps identify the dominant themes in each document and understand how content is distributed across topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14ddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "The stock market closed higher today as tech shares rallied amid strong earnings reports.\n",
      "Topic Distribution:\n",
      "  Topic 6: 0.9327\n",
      "\n",
      "Document 2:\n",
      "A major earthquake struck the coastal city early this morning, causing widespread damage.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0114\n",
      "  Topic 1: 0.0114\n",
      "  Topic 2: 0.0114\n",
      "  Topic 3: 0.0114\n",
      "  Topic 4: 0.0114\n",
      "  Topic 5: 0.9205\n",
      "  Topic 6: 0.0114\n",
      "  Topic 7: 0.0114\n",
      "\n",
      "Document 3:\n",
      "The government announced new policies aimed at reducing carbon emissions by 2030.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0139\n",
      "  Topic 1: 0.0139\n",
      "  Topic 2: 0.0139\n",
      "  Topic 3: 0.0139\n",
      "  Topic 4: 0.0139\n",
      "  Topic 5: 0.0139\n",
      "  Topic 6: 0.0139\n",
      "  Topic 7: 0.9028\n",
      "\n",
      "Document 4:\n",
      "Scientists discovered a new species of dinosaur in the remote mountains of Argentina.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0139\n",
      "  Topic 1: 0.0139\n",
      "  Topic 2: 0.0139\n",
      "  Topic 3: 0.0139\n",
      "  Topic 4: 0.0139\n",
      "  Topic 5: 0.0139\n",
      "  Topic 6: 0.0139\n",
      "  Topic 7: 0.9028\n",
      "\n",
      "Document 5:\n",
      "The local football team won the championship after a thrilling final match.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0156\n",
      "  Topic 1: 0.0156\n",
      "  Topic 2: 0.0156\n",
      "  Topic 3: 0.0156\n",
      "  Topic 4: 0.0156\n",
      "  Topic 5: 0.0156\n",
      "  Topic 6: 0.0156\n",
      "  Topic 7: 0.8906\n",
      "\n",
      "Document 6:\n",
      "Health officials urge citizens to get vaccinated as flu season approaches.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0125\n",
      "  Topic 1: 0.0125\n",
      "  Topic 2: 0.0125\n",
      "  Topic 3: 0.0125\n",
      "  Topic 4: 0.0125\n",
      "  Topic 5: 0.0125\n",
      "  Topic 6: 0.0125\n",
      "  Topic 7: 0.9125\n",
      "\n",
      "Document 7:\n",
      "A breakthrough in renewable energy technology promises cheaper solar panels.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0139\n",
      "  Topic 1: 0.0139\n",
      "  Topic 2: 0.0139\n",
      "  Topic 3: 0.0139\n",
      "  Topic 4: 0.0139\n",
      "  Topic 5: 0.0139\n",
      "  Topic 6: 0.0139\n",
      "  Topic 7: 0.9028\n",
      "\n",
      "Document 8:\n",
      "International leaders met to discuss trade agreements and economic cooperation.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0139\n",
      "  Topic 1: 0.0139\n",
      "  Topic 2: 0.0139\n",
      "  Topic 3: 0.0139\n",
      "  Topic 4: 0.0139\n",
      "  Topic 5: 0.0139\n",
      "  Topic 6: 0.9028\n",
      "  Topic 7: 0.0139\n",
      "\n",
      "Document 9:\n",
      "A popular film festival opened this weekend, showcasing independent movies from around the world.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0114\n",
      "  Topic 1: 0.0114\n",
      "  Topic 2: 0.9205\n",
      "  Topic 3: 0.0114\n",
      "  Topic 4: 0.0114\n",
      "  Topic 5: 0.0114\n",
      "  Topic 6: 0.0114\n",
      "  Topic 7: 0.0114\n",
      "\n",
      "Document 10:\n",
      "The city council approved plans for a new public park to promote green spaces.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0114\n",
      "  Topic 1: 0.0114\n",
      "  Topic 2: 0.0114\n",
      "  Topic 3: 0.0114\n",
      "  Topic 4: 0.0114\n",
      "  Topic 5: 0.9205\n",
      "  Topic 6: 0.0114\n",
      "  Topic 7: 0.0114\n"
     ]
    }
   ],
   "source": [
    "# Get topic distributions for all documents in the BoW corpus\n",
    "doc_topics = [best_model.get_document_topics(doc) for doc in bow_corpus]\n",
    "\n",
    "# Print original document with its topic distribution\n",
    "for i in range(len(bow_corpus)):\n",
    "    print(f\"\\nDocument {i + 1}:\\n{corpus[i]}\")\n",
    "    print(\"Topic Distribution:\")\n",
    "    for topic_id, prob in doc_topics[i]:\n",
    "        print(f\"  Topic {topic_id}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6759de",
   "metadata": {},
   "source": [
    "## Identify Dominant Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a77cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: Dominant Topic = 6, Score = 0.9327\n",
      "Document 2: Dominant Topic = 5, Score = 0.9205\n",
      "Document 3: Dominant Topic = 7, Score = 0.9028\n",
      "Document 4: Dominant Topic = 7, Score = 0.9028\n",
      "Document 5: Dominant Topic = 7, Score = 0.8906\n",
      "Document 6: Dominant Topic = 7, Score = 0.9125\n",
      "Document 7: Dominant Topic = 7, Score = 0.9028\n",
      "Document 8: Dominant Topic = 6, Score = 0.9028\n",
      "Document 9: Dominant Topic = 2, Score = 0.9205\n",
      "Document 10: Dominant Topic = 5, Score = 0.9205\n"
     ]
    }
   ],
   "source": [
    "# Identify the dominant topic in each document\n",
    "dominant_topics = []\n",
    "for i, topics in enumerate(doc_topics):\n",
    "    if topics:\n",
    "        # Get the topic with the highest probability\n",
    "        dominant_topic = max(topics, key=lambda x: x[1])\n",
    "        dominant_topics.append((i, dominant_topic[0], dominant_topic[1]))\n",
    "        print(f\"Document {i+1}: Dominant Topic = {dominant_topic[0]}, Score = {dominant_topic[1]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Document {i+1}: No dominant topic found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526ff9f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
