{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d23103d",
   "metadata": {},
   "source": [
    "# Topic Modeling on News Snippets Using LDA\n",
    "**Author:** Virginia Herrero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d8b69",
   "metadata": {},
   "source": [
    "## Import Libraries and Download Resources\n",
    "\n",
    "Import essential libraries for text preprocessing, topic modeling, and download required NLTK resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b893e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Text processing\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Topic Modeling\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaMulticore, CoherenceModel\n",
    "\n",
    "# Utilities\n",
    "from pprint import pprint\n",
    "from pprint import pformat\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31ce73",
   "metadata": {},
   "source": [
    "## Define the Corpus\n",
    "\n",
    "In natural language processing, a corpus is a collection of written or spoken texts that serves as the dataset for language-related tasks. The corpus is analyzed to identify language patterns and typically requires preprocessing and transformation into a format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62967e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The stock market closed higher today as tech shares rallied amid strong earnings reports.\",\n",
    "    \"A major earthquake struck the coastal city early this morning, causing widespread damage.\",\n",
    "    \"The government announced new policies aimed at reducing carbon emissions by 2030.\",\n",
    "    \"Scientists discovered a new species of dinosaur in the remote mountains of Argentina.\",\n",
    "    \"The local football team won the championship after a thrilling final match.\",\n",
    "    \"Health officials urge citizens to get vaccinated as flu season approaches.\",\n",
    "    \"A breakthrough in renewable energy technology promises cheaper solar panels.\",\n",
    "    \"International leaders met to discuss trade agreements and economic cooperation.\",\n",
    "    \"A popular film festival opened this weekend, showcasing independent movies from around the world.\",\n",
    "    \"The city council approved plans for a new public park to promote green spaces.\",\n",
    "    \n",
    "    \"Tech startups attracted record venture capital funding in the last quarter.\",\n",
    "    \"Severe flooding affected thousands as the hurricane swept across the southern coast.\",\n",
    "    \"New education reforms aim to improve literacy rates in rural areas.\",\n",
    "    \"Researchers developed a vaccine candidate showing promise against the new virus strain.\",\n",
    "    \"The basketball team secured a playoff spot after a narrow victory last night.\",\n",
    "    \"Authorities launched a campaign to combat misinformation on social media platforms.\",\n",
    "    \"Advancements in artificial intelligence are reshaping the manufacturing sector.\",\n",
    "    \"A new art exhibition opened downtown featuring contemporary local artists.\",\n",
    "    \"The mayor announced plans to improve public transportation infrastructure.\",\n",
    "    \"Farmers are adopting sustainable practices to tackle climate change impacts.\",\n",
    "    \n",
    "    \"Oil prices surged following geopolitical tensions in the Middle East.\",\n",
    "    \"New study reveals the impact of microplastics on marine ecosystems.\",\n",
    "    \"The annual marathon attracted thousands of runners from across the globe.\",\n",
    "    \"Police investigated a cyberattack targeting the financial sector.\",\n",
    "    \"Renewed peace talks offered hope for conflict resolution in the region.\",\n",
    "    \"Wildlife conservation efforts led to an increase in endangered species populations.\",\n",
    "    \"The tech giant released its latest smartphone model with advanced features.\",\n",
    "    \"Local businesses reported strong sales during the holiday shopping season.\",\n",
    "    \"Scientists are exploring the potential of quantum computing for drug discovery.\",\n",
    "    \"Public health officials responded to a surge in measles cases this year.\",\n",
    "    \n",
    "    \"New legislation passed to support small businesses and entrepreneurs.\",\n",
    "    \"Astronomers detected signals from a distant galaxy that may indicate life.\",\n",
    "    \"The national soccer team prepared for the upcoming international tournament.\",\n",
    "    \"Electric vehicle sales doubled as more charging stations became available.\",\n",
    "    \"A documentary highlighted the challenges faced by indigenous communities.\",\n",
    "    \"The finance ministry announced a budget increase for infrastructure projects.\",\n",
    "    \"Researchers tested new water purification methods in urban areas.\",\n",
    "    \"The local theater troupe received accolades for their latest production.\",\n",
    "    \"Emergency responders worked around the clock after the wildfires spread.\",\n",
    "    \"Scientists mapped the genome of a rare plant species found in the Amazon.\",\n",
    "    \n",
    "    \"The central bank adjusted interest rates to stabilize the economy.\",\n",
    "    \"New regulations aim to reduce plastic waste in oceans and rivers.\",\n",
    "    \"Community volunteers organized a cleanup drive at the city park.\",\n",
    "    \"The education department launched an online learning platform for students.\",\n",
    "    \"Sports officials confirmed the cancellation of events due to the pandemic.\",\n",
    "    \"Renewable energy projects received funding from international organizations.\",\n",
    "    \"New smartphone apps are helping users monitor their mental health.\",\n",
    "    \"The local library hosted a workshop on digital literacy for seniors.\",\n",
    "    \"Authorities investigated allegations of corruption in government contracts.\",\n",
    "    \"Advances in battery technology promise longer-lasting electric cars.\",\n",
    "    \n",
    "    \"A cultural festival celebrated traditional music and dance from the region.\",\n",
    "    \"The airline industry faced challenges as travel restrictions eased.\",\n",
    "    \"Researchers discovered a potential treatment for Alzheimer's disease.\",\n",
    "    \"The city hosted an international conference on climate change mitigation.\",\n",
    "    \"New cybersecurity measures were implemented to protect critical infrastructure.\",\n",
    "    \"Farmers reported better yields thanks to innovative irrigation techniques.\",\n",
    "    \"The national chess champion defended her title in a tense match.\",\n",
    "    \"Scientists studied the effects of air pollution on respiratory health.\",\n",
    "    \"Public transportation ridership increased following fare reductions.\",\n",
    "    \"The art museum unveiled a collection of rare historical artifacts.\",\n",
    "    \n",
    "    \"Government officials discussed strategies to boost tourism post-pandemic.\",\n",
    "    \"The tech sector created thousands of new jobs in the last fiscal year.\",\n",
    "    \"Community health clinics expanded services to underserved neighborhoods.\",\n",
    "    \"New research sheds light on the origins of ancient civilizations.\",\n",
    "    \"Environmental groups campaigned against deforestation in protected areas.\",\n",
    "    \"The football league introduced new rules to enhance player safety.\",\n",
    "    \"The startup developed a platform for remote team collaboration.\",\n",
    "    \"Schools implemented measures to improve student well-being and mental health.\",\n",
    "    \"The film director received awards at several international festivals.\",\n",
    "    \"Scientists launched a satellite to monitor global climate patterns.\",\n",
    "    \n",
    "    \"The city council debated zoning changes to accommodate new housing developments.\",\n",
    "    \"Electric buses were introduced to reduce urban air pollution.\",\n",
    "    \"Researchers analyzed data from recent volcanic activity to predict eruptions.\",\n",
    "    \"A charity event raised funds to support homeless shelters during winter.\",\n",
    "    \"The government increased investment in renewable energy infrastructure.\",\n",
    "    \"Tech companies collaborated on standards for data privacy protection.\",\n",
    "    \"The wildlife reserve reported a surge in tourist visits this season.\",\n",
    "    \"Community gardens flourished as residents embraced urban agriculture.\",\n",
    "    \"A new study explored the psychological effects of social media use.\",\n",
    "    \"Officials launched an initiative to promote literacy among children.\",\n",
    "    \n",
    "    \"The national park expanded trails to encourage eco-tourism.\",\n",
    "    \"Local authorities improved emergency preparedness for natural disasters.\",\n",
    "    \"The sports federation held training camps ahead of the championship.\",\n",
    "    \"Scientists developed biodegradable materials to reduce plastic pollution.\",\n",
    "    \"The government passed reforms to improve healthcare access nationwide.\",\n",
    "    \"Entrepreneurs launched innovative fintech solutions for rural markets.\",\n",
    "    \"The city experienced a heatwave causing health warnings for vulnerable populations.\",\n",
    "    \"Artists collaborated on a mural celebrating cultural diversity.\",\n",
    "    \"Renewable energy sources accounted for a record percentage of electricity generation.\",\n",
    "    \"Authorities investigated fraud in recent public procurement contracts.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e0d46",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "\n",
    "After defining the corpus, the next step is text preprocessing. This step involves cleaning and preparing the raw text data to make it suitable for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94420aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stopwords\n",
    "stop_w = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1842f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "def doc_to_tokens(texts):\n",
    "    \"\"\"\n",
    "    Tokenize a list of documents into clean lowercase words.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    texts (list of str): List of raw text documents.\n",
    "\n",
    "    Yields:\n",
    "    ----------\n",
    "    list of str: Tokenized and lowercased words from each document,\n",
    "                 with punctuation removed.\n",
    "    \"\"\"\n",
    "    for doc in texts:\n",
    "        yield simple_preprocess(doc, deacc=True)\n",
    "\n",
    "tokens = list(doc_to_tokens(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e9faa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "def rm_stopwords(docs):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from tokenized documents.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    docs (list of list of str): Tokenized documents (list of words).\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    list of list of str: Tokenized documents with stopwords removed.\n",
    "    \"\"\"\n",
    "    return [[word for word in doc if word not in stop_w] for doc in docs]\n",
    "\n",
    "tokens = rm_stopwords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07db4ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed documents:\n",
      "[['stock', 'market', 'close', 'high', 'today', 'tech', 'share', 'rally', 'amid', 'strong', 'earnings', 'report'], ['major', 'earthquake', 'strike', 'coastal', 'city', 'early', 'morning', 'cause', 'widespread', 'damage']]\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convert TreeBank POS tags (from NLTK's pos_tag) to WordNet POS tags,\n",
    "    which are required for accurate lemmatization.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun if unknown\n",
    "\n",
    "def preprocess_lemmatize(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatizes a list of tokenized documents using POS-aware lemmatization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list of list of str\n",
    "        List of tokenized documents. Each document is a list of word tokens.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    docs : list of list of str\n",
    "        List of lemmatized documents. Each document is a list of lemmatized word tokens.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = []\n",
    "\n",
    "    for doc in tokens:\n",
    "        # Tag each token with its part of speech (POS)\n",
    "        tagged_tokens = pos_tag(doc)\n",
    "\n",
    "        # Lemmatize each token using its POS tag\n",
    "        lemmatized_doc = [\n",
    "            lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
    "            for token, pos in tagged_tokens\n",
    "        ]\n",
    "        docs.append(lemmatized_doc)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "# Preprocess the tokens\n",
    "docs = preprocess_lemmatize(tokens)\n",
    "\n",
    "# Print first two processed documents\n",
    "print(\"Sample processed documents:\")\n",
    "print(docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913bfee",
   "metadata": {},
   "source": [
    "## Create Dictionary\n",
    "\n",
    "A dictionary in natural language processing is a mapping between unique words (tokens) in the corpus and their integer IDs. It serves as a vocabulary reference that converts text data into numerical formats required by machine learning models. In topic modeling, the dictionary helps translate words into a consistent numeric representation used to build the corpus and train models like LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c323aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dictionary token-id pairs:\n",
      "[(0, 'amid'), (1, 'close'), (2, 'earnings'), (3, 'high'), (4, 'market'), (5, 'rally'), (6, 'report'), (7, 'share'), (8, 'stock'), (9, 'strong')]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary representation of the documents\n",
    "word_dict = corpora.Dictionary(docs)\n",
    "\n",
    "# Print the first 10 token-id\n",
    "print(\"Sample dictionary token-id pairs:\")\n",
    "print(list(word_dict.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29984f8",
   "metadata": {},
   "source": [
    "## Create Bag-of-Words\n",
    "\n",
    "A bag of words (BoW) is a simple and commonly used method for representing text data in natural language processing. It treats a document as a \"bag\" of individual words, ignoring grammar and word order, but keeping track of how many times each word appears. Each document is converted into a vector of word counts based on a predefined vocabulary. In conclusion, a bag of words is a numerical representation of text that captures word frequency, used to feed text data into machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4aadfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample bag-of-words representation for first document:\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create the bag-of-words corpus\n",
    "bow_corpus = [word_dict.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# Print the bag-of-words for the first document\n",
    "print(\"Sample bag-of-words representation for first document:\")\n",
    "print(bow_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb756be0",
   "metadata": {},
   "source": [
    "## LDA Model\n",
    "\n",
    "Topic modeling is the process of uncovering hidden thematic structures in a collection of documents. LDA (Latent Dirichlet Allocation) is one of the most commonly used algorithms for this task. It identifies groups of words that frequently occur together and uses them to define topics, allowing each document to be represented as a mixture of these topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0cded99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"launch\" + 0.011*\"new\" + 0.009*\"scientist\" + 0.009*\"official\" + '\n",
      "  '0.009*\"platform\" + 0.009*\"pollution\" + 0.009*\"local\" + 0.009*\"team\" + '\n",
      "  '0.006*\"government\" + 0.006*\"improve\"'),\n",
      " (1,\n",
      "  '0.013*\"new\" + 0.008*\"tech\" + 0.008*\"energy\" + 0.008*\"renewable\" + '\n",
      "  '0.008*\"community\" + 0.008*\"thousand\" + 0.008*\"health\" + 0.006*\"local\" + '\n",
      "  '0.006*\"report\" + 0.006*\"last\"'),\n",
      " (2,\n",
      "  '0.023*\"new\" + 0.017*\"city\" + 0.015*\"public\" + 0.009*\"aim\" + '\n",
      "  '0.009*\"announce\" + 0.009*\"investigate\" + 0.007*\"health\" + 0.007*\"authority\" '\n",
      "  '+ 0.007*\"researcher\" + 0.006*\"government\"')]\n"
     ]
    }
   ],
   "source": [
    "# Train the LDA model\n",
    "lda_model = LdaMulticore(\n",
    "    corpus = bow_corpus,        # The BoW representation of the documents\n",
    "    id2word = word_dict,        # The dictionary mapping of word IDs\n",
    "    num_topics = 3,             # Number of topics to extract\n",
    "    random_state = 42,          # For reproducibility\n",
    "    passes = 10,                # Number of passes through the corpus during training\n",
    ")\n",
    "\n",
    "# Display the discovered topics\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics(num_words = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2f1f0",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Model evaluation measures how well a topic model produces coherent and meaningful topics. This process ensures the model’s results are reliable and interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090b13cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6217\n"
     ]
    }
   ],
   "source": [
    "# Build coherence model\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model = lda_model, \n",
    "    texts = docs,          \n",
    "    dictionary = word_dict,\n",
    "    coherence = \"c_v\"      \n",
    ")\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "\n",
    "print(f\"Coherence Score: {coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8e4d0",
   "metadata": {},
   "source": [
    "The model’s coherence score is quite good, nonetheless the next step is to train multiple LDA models using different numbers of topics and calculate their coherence scores. By comparing these scores, it finds the optimal number of topics that improves the model’s coherence and overall quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224877e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple LDA models to find the best\n",
    "def compute_coherence_values(dictionary, corpus, texts, start = 2, limit = 10, step = 1):\n",
    "    \"\"\"\n",
    "    Train LDA models with varying number of topics and compute their coherence scores.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    - dictionary (gensim.corpora.Dictionary): Mapping of word IDs to words.\n",
    "    - corpus (list of list of (int, int)): Bag-of-words representation of documents.\n",
    "    - texts (list of list of str): Preprocessed tokenized documents.\n",
    "    - start (int): Minimum number of topics to try (inclusive).\n",
    "    - limit (int): Maximum number of topics to try (inclusive).\n",
    "    - step (int): Step size between topic numbers.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    - model_list (list): List of trained LDA models.\n",
    "    - coherence_values (list): List of coherence scores corresponding to each model.\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, limit + 1, step):\n",
    "        print(f\"Training LDA with {num_topics} topics...\")\n",
    "        model = LdaMulticore(\n",
    "            corpus = corpus,\n",
    "            id2word = dictionary,\n",
    "            num_topics = num_topics,\n",
    "            random_state = 42,\n",
    "            passes = 10,\n",
    "        )\n",
    "        model_list.append(model)\n",
    "        \n",
    "        coherence_model = CoherenceModel(\n",
    "            model = model,\n",
    "            texts = texts,\n",
    "            dictionary = dictionary,\n",
    "            coherence = \"c_v\"\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        coherence_values.append(coherence_score)\n",
    "        \n",
    "        print(f\"Coherence Score for {num_topics} topics: {coherence_score:.4f}\\n\")\n",
    "    \n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b9db36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA with 2 topics...\n",
      "Coherence Score for 2 topics: 0.6156\n",
      "\n",
      "Training LDA with 3 topics...\n",
      "Coherence Score for 3 topics: 0.6217\n",
      "\n",
      "Training LDA with 4 topics...\n",
      "Coherence Score for 4 topics: 0.6119\n",
      "\n",
      "Training LDA with 5 topics...\n",
      "Coherence Score for 5 topics: 0.6164\n",
      "\n",
      "Training LDA with 6 topics...\n",
      "Coherence Score for 6 topics: 0.5774\n",
      "\n",
      "Training LDA with 7 topics...\n",
      "Coherence Score for 7 topics: 0.5817\n",
      "\n",
      "Training LDA with 8 topics...\n",
      "Coherence Score for 8 topics: 0.5387\n",
      "\n",
      "Training LDA with 9 topics...\n",
      "Coherence Score for 9 topics: 0.5258\n",
      "\n",
      "Training LDA with 10 topics...\n",
      "Coherence Score for 10 topics: 0.4725\n",
      "\n",
      "Num Topics = 2 => Coherence Score = 0.6156\n",
      "Num Topics = 3 => Coherence Score = 0.6217\n",
      "Num Topics = 4 => Coherence Score = 0.6119\n",
      "Num Topics = 5 => Coherence Score = 0.6164\n",
      "Num Topics = 6 => Coherence Score = 0.5774\n",
      "Num Topics = 7 => Coherence Score = 0.5817\n",
      "Num Topics = 8 => Coherence Score = 0.5387\n",
      "Num Topics = 9 => Coherence Score = 0.5258\n",
      "Num Topics = 10 => Coherence Score = 0.4725\n"
     ]
    }
   ],
   "source": [
    "# Run coherence evaluation for topics 2 to 10\n",
    "model_list, coherence_values = compute_coherence_values(word_dict, bow_corpus, docs, start = 2, limit = 10, step = 1)\n",
    "\n",
    "# Print summary\n",
    "for num, score in zip(range(2, 11), coherence_values):\n",
    "    print(f\"Num Topics = {num} => Coherence Score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "585bf677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the coherence scores\n",
    "with open(\"../results/metrics/lda-coherence-scores.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([f\"Num Topics = {num} => Coherence Score = {score:.4f}\" \n",
    "                       for num, score in zip(range(2, 11), coherence_values)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75530b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LDA Model has 3 topics with coherence score 0.6217\n",
      "\n",
      "[(0,\n",
      "  '0.012*\"launch\" + 0.011*\"new\" + 0.009*\"scientist\" + 0.009*\"official\" + '\n",
      "  '0.009*\"platform\" + 0.009*\"pollution\" + 0.009*\"local\" + 0.009*\"team\" + '\n",
      "  '0.006*\"government\" + 0.006*\"improve\" + 0.006*\"study\" + 0.006*\"national\" + '\n",
      "  '0.006*\"urban\" + 0.006*\"festival\" + 0.006*\"electric\" + 0.006*\"literacy\" + '\n",
      "  '0.006*\"international\" + 0.006*\"develop\" + 0.006*\"introduce\" + 0.006*\"air\"'),\n",
      " (1,\n",
      "  '0.013*\"new\" + 0.008*\"tech\" + 0.008*\"energy\" + 0.008*\"renewable\" + '\n",
      "  '0.008*\"community\" + 0.008*\"thousand\" + 0.008*\"health\" + 0.006*\"local\" + '\n",
      "  '0.006*\"report\" + 0.006*\"last\" + 0.006*\"season\" + 0.006*\"sector\" + '\n",
      "  '0.006*\"specie\" + 0.006*\"expand\" + 0.006*\"protect\" + 0.006*\"emergency\" + '\n",
      "  '0.006*\"entrepreneur\" + 0.006*\"across\" + 0.006*\"market\" + 0.006*\"business\"'),\n",
      " (2,\n",
      "  '0.023*\"new\" + 0.017*\"city\" + 0.015*\"public\" + 0.009*\"aim\" + '\n",
      "  '0.009*\"announce\" + 0.009*\"investigate\" + 0.007*\"health\" + 0.007*\"authority\" '\n",
      "  '+ 0.007*\"researcher\" + 0.006*\"government\" + 0.006*\"improve\" + '\n",
      "  '0.006*\"promise\" + 0.006*\"park\" + 0.006*\"receive\" + 0.006*\"change\" + '\n",
      "  '0.006*\"international\" + 0.006*\"infrastructure\" + 0.006*\"increase\" + '\n",
      "  '0.006*\"scientist\" + 0.006*\"reduce\"')]\n"
     ]
    }
   ],
   "source": [
    "# The best model\n",
    "# Find the index of the best coherence score\n",
    "best_model_index = coherence_values.index(max(coherence_values))\n",
    "\n",
    "# Select the best model\n",
    "best_model = model_list[best_model_index]\n",
    "\n",
    "# Print summary of the best model\n",
    "print(f\"\\nBest LDA Model has {best_model.num_topics} topics with coherence score {coherence_values[best_model_index]:.4f}\\n\")\n",
    "\n",
    "# Pretty-print the topics of the best model\n",
    "pprint(best_model.print_topics(num_words = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8576b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model results as a text file\n",
    "with open(\"../results/metrics/best-lda-model.txt\", \"w\") as f:\n",
    "    f.write(f\"Best LDA Model: {best_model.num_topics} topics\\n\")\n",
    "    f.write(f\"Coherence Score: {coherence_values[best_model_index]:.4f}\\n\\n\")\n",
    "    f.write(pformat(best_model.print_topics(num_words = 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d710489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained best model\n",
    "best_model.save(\"../model/best-lda-model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196af62e",
   "metadata": {},
   "source": [
    "The model was tested with different numbers of topics, and the quality of the topics improved as the number increased, peaking at 3 topics with the best coherence score of 0.6217. This means the model found the most meaningful and distinct themes when using 3 topics. The topics include groups of related words representing different themes like environment, finance, and health. While the score shows the model captures some clear patterns, there is still room to improve the results with further tuning or preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1ef7d",
   "metadata": {},
   "source": [
    "## Extract Document Topic Distributions\n",
    "\n",
    "The topic distribution shows how much each topic contributes to a given document. After training the LDA model, each document is represented as a mixture of topics with associated probabilities. This helps identify the dominant themes in each document and understand how content is distributed across topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe14ddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "The stock market closed higher today as tech shares rallied amid strong earnings reports.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0260\n",
      "  Topic 1: 0.9479\n",
      "  Topic 2: 0.0260\n",
      "\n",
      "Document 2:\n",
      "A major earthquake struck the coastal city early this morning, causing widespread damage.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0305\n",
      "  Topic 1: 0.0305\n",
      "  Topic 2: 0.9391\n",
      "\n",
      "Document 3:\n",
      "The government announced new policies aimed at reducing carbon emissions by 2030.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0389\n",
      "  Topic 1: 0.0378\n",
      "  Topic 2: 0.9233\n",
      "\n",
      "Document 4:\n",
      "Scientists discovered a new species of dinosaur in the remote mountains of Argentina.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0394\n",
      "  Topic 1: 0.0393\n",
      "  Topic 2: 0.9212\n",
      "\n",
      "Document 5:\n",
      "The local football team won the championship after a thrilling final match.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9151\n",
      "  Topic 1: 0.0428\n",
      "  Topic 2: 0.0421\n",
      "\n",
      "Document 6:\n",
      "Health officials urge citizens to get vaccinated as flu season approaches.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0353\n",
      "  Topic 1: 0.9308\n",
      "  Topic 2: 0.0340\n",
      "\n",
      "Document 7:\n",
      "A breakthrough in renewable energy technology promises cheaper solar panels.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0377\n",
      "  Topic 1: 0.9223\n",
      "  Topic 2: 0.0401\n",
      "\n",
      "Document 8:\n",
      "International leaders met to discuss trade agreements and economic cooperation.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0399\n",
      "  Topic 1: 0.9210\n",
      "  Topic 2: 0.0390\n",
      "\n",
      "Document 9:\n",
      "A popular film festival opened this weekend, showcasing independent movies from around the world.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9380\n",
      "  Topic 1: 0.0309\n",
      "  Topic 2: 0.0311\n",
      "\n",
      "Document 10:\n",
      "The city council approved plans for a new public park to promote green spaces.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0310\n",
      "  Topic 1: 0.0308\n",
      "  Topic 2: 0.9382\n",
      "\n",
      "Document 11:\n",
      "Tech startups attracted record venture capital funding in the last quarter.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9260\n",
      "  Topic 1: 0.0393\n",
      "  Topic 2: 0.0347\n",
      "\n",
      "Document 12:\n",
      "Severe flooding affected thousands as the hurricane swept across the southern coast.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0335\n",
      "  Topic 1: 0.9329\n",
      "  Topic 2: 0.0336\n",
      "\n",
      "Document 13:\n",
      "New education reforms aim to improve literacy rates in rural areas.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0384\n",
      "  Topic 1: 0.0357\n",
      "  Topic 2: 0.9260\n",
      "\n",
      "Document 14:\n",
      "Researchers developed a vaccine candidate showing promise against the new virus strain.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0352\n",
      "  Topic 1: 0.0342\n",
      "  Topic 2: 0.9305\n",
      "\n",
      "Document 15:\n",
      "The basketball team secured a playoff spot after a narrow victory last night.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0360\n",
      "  Topic 1: 0.9302\n",
      "  Topic 2: 0.0338\n",
      "\n",
      "Document 16:\n",
      "Authorities launched a campaign to combat misinformation on social media platforms.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9209\n",
      "  Topic 1: 0.0403\n",
      "  Topic 2: 0.0388\n",
      "\n",
      "Document 17:\n",
      "Advancements in artificial intelligence are reshaping the manufacturing sector.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0479\n",
      "  Topic 1: 0.9035\n",
      "  Topic 2: 0.0485\n",
      "\n",
      "Document 18:\n",
      "A new art exhibition opened downtown featuring contemporary local artists.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9300\n",
      "  Topic 1: 0.0351\n",
      "  Topic 2: 0.0349\n",
      "\n",
      "Document 19:\n",
      "The mayor announced plans to improve public transportation infrastructure.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0426\n",
      "  Topic 1: 0.0429\n",
      "  Topic 2: 0.9145\n",
      "\n",
      "Document 20:\n",
      "Farmers are adopting sustainable practices to tackle climate change impacts.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0388\n",
      "  Topic 1: 0.9198\n",
      "  Topic 2: 0.0414\n",
      "\n",
      "Document 21:\n",
      "Oil prices surged following geopolitical tensions in the Middle East.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0376\n",
      "  Topic 1: 0.0374\n",
      "  Topic 2: 0.9250\n",
      "\n",
      "Document 22:\n",
      "New study reveals the impact of microplastics on marine ecosystems.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9117\n",
      "  Topic 1: 0.0443\n",
      "  Topic 2: 0.0440\n",
      "\n",
      "Document 23:\n",
      "The annual marathon attracted thousands of runners from across the globe.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0428\n",
      "  Topic 1: 0.9152\n",
      "  Topic 2: 0.0419\n",
      "\n",
      "Document 24:\n",
      "Police investigated a cyberattack targeting the financial sector.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0479\n",
      "  Topic 1: 0.0503\n",
      "  Topic 2: 0.9018\n",
      "\n",
      "Document 25:\n",
      "Renewed peace talks offered hope for conflict resolution in the region.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0380\n",
      "  Topic 1: 0.9247\n",
      "  Topic 2: 0.0373\n",
      "\n",
      "Document 26:\n",
      "Wildlife conservation efforts led to an increase in endangered species populations.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0380\n",
      "  Topic 1: 0.9227\n",
      "  Topic 2: 0.0393\n",
      "\n",
      "Document 27:\n",
      "The tech giant released its latest smartphone model with advanced features.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0375\n",
      "  Topic 1: 0.9223\n",
      "  Topic 2: 0.0402\n",
      "\n",
      "Document 28:\n",
      "Local businesses reported strong sales during the holiday shopping season.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0401\n",
      "  Topic 1: 0.9220\n",
      "  Topic 2: 0.0379\n",
      "\n",
      "Document 29:\n",
      "Scientists are exploring the potential of quantum computing for drug discovery.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0432\n",
      "  Topic 1: 0.0430\n",
      "  Topic 2: 0.9138\n",
      "\n",
      "Document 30:\n",
      "Public health officials responded to a surge in measles cases this year.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0412\n",
      "  Topic 1: 0.7225\n",
      "  Topic 2: 0.2363\n",
      "\n",
      "Document 31:\n",
      "New legislation passed to support small businesses and entrepreneurs.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0436\n",
      "  Topic 1: 0.9115\n",
      "  Topic 2: 0.0449\n",
      "\n",
      "Document 32:\n",
      "Astronomers detected signals from a distant galaxy that may indicate life.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9254\n",
      "  Topic 1: 0.0373\n",
      "  Topic 2: 0.0373\n",
      "\n",
      "Document 33:\n",
      "The national soccer team prepared for the upcoming international tournament.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9144\n",
      "  Topic 1: 0.0429\n",
      "  Topic 2: 0.0428\n",
      "\n",
      "Document 34:\n",
      "Electric vehicle sales doubled as more charging stations became available.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9246\n",
      "  Topic 1: 0.0379\n",
      "  Topic 2: 0.0376\n",
      "\n",
      "Document 35:\n",
      "A documentary highlighted the challenges faced by indigenous communities.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0507\n",
      "  Topic 1: 0.9010\n",
      "  Topic 2: 0.0483\n",
      "\n",
      "Document 36:\n",
      "The finance ministry announced a budget increase for infrastructure projects.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0427\n",
      "  Topic 1: 0.0435\n",
      "  Topic 2: 0.9138\n",
      "\n",
      "Document 37:\n",
      "Researchers tested new water purification methods in urban areas.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9177\n",
      "  Topic 1: 0.0399\n",
      "  Topic 2: 0.0424\n",
      "\n",
      "Document 38:\n",
      "The local theater troupe received accolades for their latest production.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0458\n",
      "  Topic 1: 0.0447\n",
      "  Topic 2: 0.9095\n",
      "\n",
      "Document 39:\n",
      "Emergency responders worked around the clock after the wildfires spread.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0429\n",
      "  Topic 1: 0.9152\n",
      "  Topic 2: 0.0420\n",
      "\n",
      "Document 40:\n",
      "Scientists mapped the genome of a rare plant species found in the Amazon.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0415\n",
      "  Topic 1: 0.9189\n",
      "  Topic 2: 0.0395\n",
      "\n",
      "Document 41:\n",
      "The central bank adjusted interest rates to stabilize the economy.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0420\n",
      "  Topic 1: 0.9151\n",
      "  Topic 2: 0.0429\n",
      "\n",
      "Document 42:\n",
      "New regulations aim to reduce plastic waste in oceans and rivers.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0389\n",
      "  Topic 1: 0.0376\n",
      "  Topic 2: 0.9235\n",
      "\n",
      "Document 43:\n",
      "Community volunteers organized a cleanup drive at the city park.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0419\n",
      "  Topic 1: 0.0454\n",
      "  Topic 2: 0.9127\n",
      "\n",
      "Document 44:\n",
      "The education department launched an online learning platform for students.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9152\n",
      "  Topic 1: 0.0420\n",
      "  Topic 2: 0.0428\n",
      "\n",
      "Document 45:\n",
      "Sports officials confirmed the cancellation of events due to the pandemic.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9149\n",
      "  Topic 1: 0.0424\n",
      "  Topic 2: 0.0428\n",
      "\n",
      "Document 46:\n",
      "Renewable energy projects received funding from international organizations.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.8998\n",
      "  Topic 1: 0.0533\n",
      "  Topic 2: 0.0470\n",
      "\n",
      "Document 47:\n",
      "New smartphone apps are helping users monitor their mental health.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0397\n",
      "  Topic 1: 0.0391\n",
      "  Topic 2: 0.9212\n",
      "\n",
      "Document 48:\n",
      "The local library hosted a workshop on digital literacy for seniors.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9140\n",
      "  Topic 1: 0.0424\n",
      "  Topic 2: 0.0436\n",
      "\n",
      "Document 49:\n",
      "Authorities investigated allegations of corruption in government contracts.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0495\n",
      "  Topic 1: 0.0487\n",
      "  Topic 2: 0.9018\n",
      "\n",
      "Document 50:\n",
      "Advances in battery technology promise longer-lasting electric cars.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0395\n",
      "  Topic 1: 0.0407\n",
      "  Topic 2: 0.9198\n",
      "\n",
      "Document 51:\n",
      "A cultural festival celebrated traditional music and dance from the region.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9130\n",
      "  Topic 1: 0.0446\n",
      "  Topic 2: 0.0423\n",
      "\n",
      "Document 52:\n",
      "The airline industry faced challenges as travel restrictions eased.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9144\n",
      "  Topic 1: 0.0436\n",
      "  Topic 2: 0.0420\n",
      "\n",
      "Document 53:\n",
      "Researchers discovered a potential treatment for Alzheimer's disease.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0483\n",
      "  Topic 1: 0.0483\n",
      "  Topic 2: 0.9034\n",
      "\n",
      "Document 54:\n",
      "The city hosted an international conference on climate change mitigation.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0447\n",
      "  Topic 1: 0.0434\n",
      "  Topic 2: 0.9120\n",
      "\n",
      "Document 55:\n",
      "New cybersecurity measures were implemented to protect critical infrastructure.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0449\n",
      "  Topic 1: 0.9102\n",
      "  Topic 2: 0.0449\n",
      "\n",
      "Document 56:\n",
      "Farmers reported better yields thanks to innovative irrigation techniques.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0379\n",
      "  Topic 1: 0.0402\n",
      "  Topic 2: 0.9219\n",
      "\n",
      "Document 57:\n",
      "The national chess champion defended her title in a tense match.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9158\n",
      "  Topic 1: 0.0422\n",
      "  Topic 2: 0.0419\n",
      "\n",
      "Document 58:\n",
      "Scientists studied the effects of air pollution on respiratory health.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9118\n",
      "  Topic 1: 0.0447\n",
      "  Topic 2: 0.0435\n",
      "\n",
      "Document 59:\n",
      "Public transportation ridership increased following fare reductions.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0418\n",
      "  Topic 1: 0.0426\n",
      "  Topic 2: 0.9156\n",
      "\n",
      "Document 60:\n",
      "The art museum unveiled a collection of rare historical artifacts.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9154\n",
      "  Topic 1: 0.0427\n",
      "  Topic 2: 0.0419\n",
      "\n",
      "Document 61:\n",
      "Government officials discussed strategies to boost tourism post-pandemic.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9227\n",
      "  Topic 1: 0.0393\n",
      "  Topic 2: 0.0379\n",
      "\n",
      "Document 62:\n",
      "The tech sector created thousands of new jobs in the last fiscal year.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0344\n",
      "  Topic 1: 0.9303\n",
      "  Topic 2: 0.0354\n",
      "\n",
      "Document 63:\n",
      "Community health clinics expanded services to underserved neighborhoods.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0425\n",
      "  Topic 1: 0.9145\n",
      "  Topic 2: 0.0429\n",
      "\n",
      "Document 64:\n",
      "New research sheds light on the origins of ancient civilizations.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0427\n",
      "  Topic 1: 0.9137\n",
      "  Topic 2: 0.0437\n",
      "\n",
      "Document 65:\n",
      "Environmental groups campaigned against deforestation in protected areas.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0507\n",
      "  Topic 1: 0.9001\n",
      "  Topic 2: 0.0492\n",
      "\n",
      "Document 66:\n",
      "The football league introduced new rules to enhance player safety.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9232\n",
      "  Topic 1: 0.0380\n",
      "  Topic 2: 0.0388\n",
      "\n",
      "Document 67:\n",
      "The startup developed a platform for remote team collaboration.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9023\n",
      "  Topic 1: 0.0481\n",
      "  Topic 2: 0.0496\n",
      "\n",
      "Document 68:\n",
      "Schools implemented measures to improve student well-being and mental health.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9202\n",
      "  Topic 1: 0.0401\n",
      "  Topic 2: 0.0397\n",
      "\n",
      "Document 69:\n",
      "The film director received awards at several international festivals.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0471\n",
      "  Topic 1: 0.0422\n",
      "  Topic 2: 0.9107\n",
      "\n",
      "Document 70:\n",
      "Scientists launched a satellite to monitor global climate patterns.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9123\n",
      "  Topic 1: 0.0431\n",
      "  Topic 2: 0.0446\n",
      "\n",
      "Document 71:\n",
      "The city council debated zoning changes to accommodate new housing developments.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0337\n",
      "  Topic 1: 0.0340\n",
      "  Topic 2: 0.9323\n",
      "\n",
      "Document 72:\n",
      "Electric buses were introduced to reduce urban air pollution.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9147\n",
      "  Topic 1: 0.0422\n",
      "  Topic 2: 0.0432\n",
      "\n",
      "Document 73:\n",
      "Researchers analyzed data from recent volcanic activity to predict eruptions.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0380\n",
      "  Topic 1: 0.9206\n",
      "  Topic 2: 0.0414\n",
      "\n",
      "Document 74:\n",
      "A charity event raised funds to support homeless shelters during winter.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0379\n",
      "  Topic 1: 0.0378\n",
      "  Topic 2: 0.9242\n",
      "\n",
      "Document 75:\n",
      "The government increased investment in renewable energy infrastructure.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0519\n",
      "  Topic 1: 0.8919\n",
      "  Topic 2: 0.0562\n",
      "\n",
      "Document 76:\n",
      "Tech companies collaborated on standards for data privacy protection.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0427\n",
      "  Topic 1: 0.0478\n",
      "  Topic 2: 0.9094\n",
      "\n",
      "Document 77:\n",
      "The wildlife reserve reported a surge in tourist visits this season.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9064\n",
      "  Topic 1: 0.0489\n",
      "  Topic 2: 0.0447\n",
      "\n",
      "Document 78:\n",
      "Community gardens flourished as residents embraced urban agriculture.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0441\n",
      "  Topic 1: 0.9136\n",
      "  Topic 2: 0.0422\n",
      "\n",
      "Document 79:\n",
      "A new study explored the psychological effects of social media use.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0441\n",
      "  Topic 1: 0.9164\n",
      "  Topic 2: 0.0395\n",
      "\n",
      "Document 80:\n",
      "Officials launched an initiative to promote literacy among children.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9143\n",
      "  Topic 1: 0.0425\n",
      "  Topic 2: 0.0432\n",
      "\n",
      "Document 81:\n",
      "The national park expanded trails to encourage eco-tourism.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0455\n",
      "  Topic 1: 0.9102\n",
      "  Topic 2: 0.0443\n",
      "\n",
      "Document 82:\n",
      "Local authorities improved emergency preparedness for natural disasters.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0482\n",
      "  Topic 1: 0.9028\n",
      "  Topic 2: 0.0490\n",
      "\n",
      "Document 83:\n",
      "The sports federation held training camps ahead of the championship.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9162\n",
      "  Topic 1: 0.0419\n",
      "  Topic 2: 0.0419\n",
      "\n",
      "Document 84:\n",
      "Scientists developed biodegradable materials to reduce plastic pollution.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9128\n",
      "  Topic 1: 0.0421\n",
      "  Topic 2: 0.0451\n",
      "\n",
      "Document 85:\n",
      "The government passed reforms to improve healthcare access nationwide.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.9114\n",
      "  Topic 1: 0.0435\n",
      "  Topic 2: 0.0451\n",
      "\n",
      "Document 86:\n",
      "Entrepreneurs launched innovative fintech solutions for rural markets.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0480\n",
      "  Topic 1: 0.9080\n",
      "  Topic 2: 0.0440\n",
      "\n",
      "Document 87:\n",
      "The city experienced a heatwave causing health warnings for vulnerable populations.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0378\n",
      "  Topic 1: 0.0387\n",
      "  Topic 2: 0.9236\n",
      "\n",
      "Document 88:\n",
      "Artists collaborated on a mural celebrating cultural diversity.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0526\n",
      "  Topic 1: 0.8981\n",
      "  Topic 2: 0.0493\n",
      "\n",
      "Document 89:\n",
      "Renewable energy sources accounted for a record percentage of electricity generation.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0384\n",
      "  Topic 1: 0.9243\n",
      "  Topic 2: 0.0373\n",
      "\n",
      "Document 90:\n",
      "Authorities investigated fraud in recent public procurement contracts.\n",
      "Topic Distribution:\n",
      "  Topic 0: 0.0422\n",
      "  Topic 1: 0.0430\n",
      "  Topic 2: 0.9149\n"
     ]
    }
   ],
   "source": [
    "# Get topic distributions for all documents in the BoW corpus\n",
    "doc_topics = [best_model.get_document_topics(doc) for doc in bow_corpus]\n",
    "\n",
    "# Print original document with its topic distribution\n",
    "for i in range(len(bow_corpus)):\n",
    "    print(f\"\\nDocument {i + 1}:\\n{corpus[i]}\")\n",
    "    print(\"Topic Distribution:\")\n",
    "    for topic_id, prob in doc_topics[i]:\n",
    "        print(f\"  Topic {topic_id}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd9c5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save document topic distribution as text file\n",
    "with open( \"../results/documents/document-topic-distributions.txt\", \"w\") as f:\n",
    "    for i, topics in enumerate(doc_topics):\n",
    "        f.write(f\"\\nDocument {i + 1}:\\n{' '.join(docs[i])}\\n\")\n",
    "        f.write(\"Topic Distribution:\\n\")\n",
    "        for topic_id, prob in topics:\n",
    "            f.write(f\"  Topic {topic_id}: {prob:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6759de",
   "metadata": {},
   "source": [
    "## Identify Dominant Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a77cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: Dominant Topic = 1, Score = 0.9479\n",
      "Document 2: Dominant Topic = 2, Score = 0.9391\n",
      "Document 3: Dominant Topic = 2, Score = 0.9233\n",
      "Document 4: Dominant Topic = 2, Score = 0.9212\n",
      "Document 5: Dominant Topic = 0, Score = 0.9151\n",
      "Document 6: Dominant Topic = 1, Score = 0.9308\n",
      "Document 7: Dominant Topic = 1, Score = 0.9223\n",
      "Document 8: Dominant Topic = 1, Score = 0.9210\n",
      "Document 9: Dominant Topic = 0, Score = 0.9380\n",
      "Document 10: Dominant Topic = 2, Score = 0.9382\n",
      "Document 11: Dominant Topic = 0, Score = 0.9260\n",
      "Document 12: Dominant Topic = 1, Score = 0.9329\n",
      "Document 13: Dominant Topic = 2, Score = 0.9260\n",
      "Document 14: Dominant Topic = 2, Score = 0.9305\n",
      "Document 15: Dominant Topic = 1, Score = 0.9302\n",
      "Document 16: Dominant Topic = 0, Score = 0.9209\n",
      "Document 17: Dominant Topic = 1, Score = 0.9035\n",
      "Document 18: Dominant Topic = 0, Score = 0.9300\n",
      "Document 19: Dominant Topic = 2, Score = 0.9145\n",
      "Document 20: Dominant Topic = 1, Score = 0.9198\n",
      "Document 21: Dominant Topic = 2, Score = 0.9250\n",
      "Document 22: Dominant Topic = 0, Score = 0.9117\n",
      "Document 23: Dominant Topic = 1, Score = 0.9152\n",
      "Document 24: Dominant Topic = 2, Score = 0.9018\n",
      "Document 25: Dominant Topic = 1, Score = 0.9247\n",
      "Document 26: Dominant Topic = 1, Score = 0.9227\n",
      "Document 27: Dominant Topic = 1, Score = 0.9223\n",
      "Document 28: Dominant Topic = 1, Score = 0.9220\n",
      "Document 29: Dominant Topic = 2, Score = 0.9138\n",
      "Document 30: Dominant Topic = 1, Score = 0.7225\n",
      "Document 31: Dominant Topic = 1, Score = 0.9115\n",
      "Document 32: Dominant Topic = 0, Score = 0.9254\n",
      "Document 33: Dominant Topic = 0, Score = 0.9144\n",
      "Document 34: Dominant Topic = 0, Score = 0.9246\n",
      "Document 35: Dominant Topic = 1, Score = 0.9010\n",
      "Document 36: Dominant Topic = 2, Score = 0.9138\n",
      "Document 37: Dominant Topic = 0, Score = 0.9177\n",
      "Document 38: Dominant Topic = 2, Score = 0.9095\n",
      "Document 39: Dominant Topic = 1, Score = 0.9152\n",
      "Document 40: Dominant Topic = 1, Score = 0.9189\n",
      "Document 41: Dominant Topic = 1, Score = 0.9151\n",
      "Document 42: Dominant Topic = 2, Score = 0.9235\n",
      "Document 43: Dominant Topic = 2, Score = 0.9127\n",
      "Document 44: Dominant Topic = 0, Score = 0.9152\n",
      "Document 45: Dominant Topic = 0, Score = 0.9149\n",
      "Document 46: Dominant Topic = 0, Score = 0.8998\n",
      "Document 47: Dominant Topic = 2, Score = 0.9212\n",
      "Document 48: Dominant Topic = 0, Score = 0.9140\n",
      "Document 49: Dominant Topic = 2, Score = 0.9018\n",
      "Document 50: Dominant Topic = 2, Score = 0.9198\n",
      "Document 51: Dominant Topic = 0, Score = 0.9130\n",
      "Document 52: Dominant Topic = 0, Score = 0.9144\n",
      "Document 53: Dominant Topic = 2, Score = 0.9034\n",
      "Document 54: Dominant Topic = 2, Score = 0.9120\n",
      "Document 55: Dominant Topic = 1, Score = 0.9102\n",
      "Document 56: Dominant Topic = 2, Score = 0.9219\n",
      "Document 57: Dominant Topic = 0, Score = 0.9158\n",
      "Document 58: Dominant Topic = 0, Score = 0.9118\n",
      "Document 59: Dominant Topic = 2, Score = 0.9156\n",
      "Document 60: Dominant Topic = 0, Score = 0.9154\n",
      "Document 61: Dominant Topic = 0, Score = 0.9227\n",
      "Document 62: Dominant Topic = 1, Score = 0.9303\n",
      "Document 63: Dominant Topic = 1, Score = 0.9145\n",
      "Document 64: Dominant Topic = 1, Score = 0.9137\n",
      "Document 65: Dominant Topic = 1, Score = 0.9001\n",
      "Document 66: Dominant Topic = 0, Score = 0.9232\n",
      "Document 67: Dominant Topic = 0, Score = 0.9023\n",
      "Document 68: Dominant Topic = 0, Score = 0.9202\n",
      "Document 69: Dominant Topic = 2, Score = 0.9107\n",
      "Document 70: Dominant Topic = 0, Score = 0.9123\n",
      "Document 71: Dominant Topic = 2, Score = 0.9323\n",
      "Document 72: Dominant Topic = 0, Score = 0.9147\n",
      "Document 73: Dominant Topic = 1, Score = 0.9206\n",
      "Document 74: Dominant Topic = 2, Score = 0.9242\n",
      "Document 75: Dominant Topic = 1, Score = 0.8919\n",
      "Document 76: Dominant Topic = 2, Score = 0.9094\n",
      "Document 77: Dominant Topic = 0, Score = 0.9064\n",
      "Document 78: Dominant Topic = 1, Score = 0.9136\n",
      "Document 79: Dominant Topic = 1, Score = 0.9164\n",
      "Document 80: Dominant Topic = 0, Score = 0.9143\n",
      "Document 81: Dominant Topic = 1, Score = 0.9102\n",
      "Document 82: Dominant Topic = 1, Score = 0.9028\n",
      "Document 83: Dominant Topic = 0, Score = 0.9162\n",
      "Document 84: Dominant Topic = 0, Score = 0.9128\n",
      "Document 85: Dominant Topic = 0, Score = 0.9114\n",
      "Document 86: Dominant Topic = 1, Score = 0.9080\n",
      "Document 87: Dominant Topic = 2, Score = 0.9236\n",
      "Document 88: Dominant Topic = 1, Score = 0.8981\n",
      "Document 89: Dominant Topic = 1, Score = 0.9243\n",
      "Document 90: Dominant Topic = 2, Score = 0.9149\n"
     ]
    }
   ],
   "source": [
    "# Identify the dominant topic in each document\n",
    "dominant_topics = []\n",
    "for i, topics in enumerate(doc_topics):\n",
    "    if topics:\n",
    "        # Get the topic with the highest probability\n",
    "        dominant_topic = max(topics, key=lambda x: x[1])\n",
    "        dominant_topics.append((i, dominant_topic[0], dominant_topic[1]))\n",
    "        print(f\"Document {i+1}: Dominant Topic = {dominant_topic[0]}, Score = {dominant_topic[1]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Document {i+1}: No dominant topic found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d136030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dominant topics as a text file\n",
    "with open(\"../results/documents/document-dominant-topics.txt\", \"w\") as f:\n",
    "    for i, topics in enumerate(doc_topics):\n",
    "        if topics:\n",
    "            dominant_topic = max(topics, key=lambda x: x[1])\n",
    "            f.write(f\"Document {i+1}: Dominant Topic = {dominant_topic[0]}, Score = {dominant_topic[1]:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"Document {i+1}: No dominant topic found\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09be961",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "Model interpretation focuses on understanding the topics identified by the model. Each topic consists of a group of keywords that frequently appear together across documents. The steps to interpret the model involves:\n",
    "\n",
    "- **Top keywords**: Lists of the most relevant words for each topic, which help describe the main idea of that topic.\n",
    "\n",
    "- **Representative documents**: Examples of documents where a specific topic is most prominent, providing context for what the topic looks like in actual text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fffc50",
   "metadata": {},
   "source": [
    "### **Top Keywords**\n",
    "\n",
    "The top keywords of an LDA model are the most representative words for each topic, revealing the main themes discussed across the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e319e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_keywords(lda_model, num_words = 10):\n",
    "    \"\"\"\n",
    "    Extract the top keywords (terms) that define each topic in the LDA model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model : gensim.models.LdaModel\n",
    "        A trained LDA model.\n",
    "    num_words : int, optional\n",
    "        Number of top words to extract for each topic (default is 10).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with topic IDs as keys and lists of top keywords as values.\n",
    "    \"\"\"\n",
    "    topic_keywords = {}\n",
    "    for topic_id in range(lda_model.num_topics):\n",
    "        topic_keywords[topic_id] = [word for word, _ in lda_model.show_topic(topic_id, topn = num_words)]\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f0fca",
   "metadata": {},
   "source": [
    "### **Representative documets per topic**\n",
    "\n",
    "The function identifies and returns the most relevant documents for each topic in an LDA model, highlighting examples that best represent each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a1b7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_representative_docs(lda_model, corpus, texts, topn_docs=3):\n",
    "    \"\"\"\n",
    "    Identify representative documents for each topic based on the topic distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model : gensim.models.LdaModel\n",
    "        A trained LDA model.\n",
    "    corpus : list of list of (int, int)\n",
    "        Bag-of-words representation of the corpus.\n",
    "    texts : list of str\n",
    "        List of original or preprocessed document texts.\n",
    "    topn_docs : int, optional\n",
    "        Number of most representative documents to return per topic (default is 3).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with topic IDs as keys and lists of (doc_index, topic_probability) tuples as values.\n",
    "    \"\"\"\n",
    "    topic_docs = defaultdict(list)\n",
    "    for i, bow in enumerate(corpus):\n",
    "        topic_probs = lda_model.get_document_topics(bow)\n",
    "        for topic_id, prob in topic_probs:\n",
    "            topic_docs[topic_id].append((i, prob))\n",
    "\n",
    "    for topic_id in topic_docs:\n",
    "        topic_docs[topic_id] = sorted(topic_docs[topic_id], key=lambda x: -x[1])[:topn_docs]\n",
    "\n",
    "    return topic_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d17ed",
   "metadata": {},
   "source": [
    "### **Topic Interpretation**\n",
    "\n",
    "Using the helper functions from before, the main function shows each topic with its top keywords and a few example documents, helping to better understand what each topic means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4f920d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_topics(lda_model, corpus, texts, num_words = 10, topn_docs = 3, save_path = None):\n",
    "    \"\"\"\n",
    "    Display an interpretation of each topic by printing its top keywords and sample representative documents.\n",
    "    Optionally saves the topic keywords to a JSON file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model : gensim.models.LdaModel\n",
    "        A trained LDA model.\n",
    "    corpus : list of list of (int, int)\n",
    "        Bag-of-words representation of the corpus.\n",
    "    texts : list of str or list of list of str\n",
    "        List of original or preprocessed document texts, either as strings or lists of tokens.\n",
    "    num_words : int, optional\n",
    "        Number of top keywords to display per topic (default is 10).\n",
    "    topn_docs : int, optional\n",
    "        Number of sample documents to show per topic (default is 3).\n",
    "    save_path : str or None, optional\n",
    "        If provided, will save topic keywords to a JSON file with the given prefix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Outputs are printed to the notebook console. If `save_path` is given, a JSON file is saved.\n",
    "    \"\"\"\n",
    "    topic_keywords = get_topic_keywords(lda_model, num_words)\n",
    "    topic_docs = get_topic_representative_docs(lda_model, corpus, texts, topn_docs)\n",
    "\n",
    "    for topic_id in range(lda_model.num_topics):\n",
    "        print(f\"\\nTopic {topic_id}: {', '.join(topic_keywords[topic_id])}\")\n",
    "        print(\"Representative Documents:\")\n",
    "        for doc_index, prob in topic_docs[topic_id]:\n",
    "            doc_text = texts[doc_index]\n",
    "            # If doc_text is a list (tokenized), join tokens; otherwise use as is\n",
    "            if isinstance(doc_text, list):\n",
    "                snippet = \" \".join(doc_text)[:300].replace(\"\\n\", \" \")\n",
    "            else:\n",
    "                snippet = doc_text[:300].replace(\"\\n\", \" \")\n",
    "            print(f\"Prob: {prob:.2f} | {snippet}\")\n",
    "        print('-' * 80)\n",
    "\n",
    "    if save_path:\n",
    "        with open(save_path, \"w\", encoding = \"utf-8\") as f:\n",
    "            json.dump(topic_keywords, f, indent = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a4294bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0: launch, new, scientist, official, platform, pollution, local, team, government, improve\n",
      "Representative Documents:\n",
      "Prob: 0.94 | popular film festival open weekend showcasing independent movie around world\n",
      "Prob: 0.93 | new art exhibition open downtown featuring contemporary local artist\n",
      "Prob: 0.93 | tech startup attract record venture capital funding last quarter\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Topic 1: new, tech, energy, renewable, community, thousand, health, local, report, last\n",
      "Representative Documents:\n",
      "Prob: 0.95 | stock market close high today tech share rally amid strong earnings report\n",
      "Prob: 0.93 | severe flooding affect thousand hurricane sweep across southern coast\n",
      "Prob: 0.93 | health official urge citizen get vaccinate flu season approach\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Topic 2: new, city, public, aim, announce, investigate, health, authority, researcher, government\n",
      "Representative Documents:\n",
      "Prob: 0.94 | major earthquake strike coastal city early morning cause widespread damage\n",
      "Prob: 0.94 | city council approve plan new public park promote green space\n",
      "Prob: 0.93 | city council debate zone change accommodate new housing development\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Interpret the topics and save top keywords\n",
    "interpret_topics(best_model, bow_corpus, docs, num_words=10, topn_docs=3, save_path=\"../results/topics/lda-model-topic-keywords.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e232e",
   "metadata": {},
   "source": [
    "### **Topic Labeling**\n",
    "\n",
    "Based on the topic interpretation results, the next step is to manually assign labels to each topic for clearer understanding. To do this, the topics’ top keywords and representative documents are reviewed and given the most appropriate labels.\n",
    "\n",
    "**Topic 0: Local Events**\n",
    "\n",
    "This topic is all about new local projects, art shows, and innovation news.\n",
    "\n",
    "- Keywords like *launch, platform, official, local, team, government* point to different initiatives and programs.\n",
    "- Representative documents focus on *events, exhibitions, startups*, and local or cultural highlights.\n",
    "\n",
    "**Topic 1: Public Affairs**\n",
    "\n",
    "This one covers big news that affects communities, especially around health, tech, and weather.\n",
    "\n",
    "- Keywords span *tech, energy, community, health, report* — all quite broad but tied to public interest.\n",
    "- Representative documents deal with *financial markets, natural disasters, and health advisories*.\n",
    "\n",
    "**Topic 2: Urban development**\n",
    "\n",
    "This topic focuses on city government, public services, and planning.\n",
    "\n",
    "- Keywords like *city, public, announce, health, authority, government* point to administrative or civic matters.\n",
    "- Representative documents focus on *earthquakes, public parks, and zoning/development decisions*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
